# -*- coding: utf-8 -*-
"""Donation Analysis for NonProfits.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aq7JOpHNF6ZU_o5NYJAZoi1XlWEmxW1z

# **Information**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/Donations/NYC City Agency Donations.csv')

"""# **1. Data Cleanup**


"""

df.info()

df.head()

# Get the number of unique values in the 'Category' column
number_of_donor = df['Name of Donor (Individual or Firm)'].nunique()

print("Number of donor:", number_of_donor)
# Get the number of unique values in the 'Category' column
number_of_agency = df['Agency Name'].nunique()

print("Number of agency:", number_of_agency)

def categorize_agency_reduced(agency_name):
    if 'Education' in agency_name:
        return 'Education'
    elif 'Cultural Affairs' in agency_name:
        return 'Cultural Affairs'
    elif any(x in agency_name for x in ['Health and Hospitals', 'Health and Mental Hygiene']):
        return 'Health Services'
    elif 'Parks' in agency_name:
        return 'Parks and Recreation'
    elif 'Borough President' in agency_name:
        return 'Borough Leadership'
    elif any(x in agency_name for x in ['Mayor', 'Public Advocate']):
        return 'City Leadership'
    elif any(x in agency_name for x in ['Police', 'Fire', 'Correction', 'Probation', 'Sanitation']):
        return 'Public Safety'
    elif any(x in agency_name for x in ['Homeless Services', 'Social Services', 'Human Resources Administration', 'Veterans', 'Administration for Children']):
        return 'Homess and Social Services and Childrens Services'
    elif any(x in agency_name for x in ['Law', 'District Attorney', 'Conflicts of Interest', 'Equal Employment', 'Civil Service']):
        return 'Legal and Law'
    elif any(x in agency_name for x in ['Housing Authority', 'Housing Preservation', 'Housing Development']):
        return 'Housing and Development'
    elif any(x in agency_name for x in ['Environmental Protection', 'Buildings', 'Landmarks', 'Records']):
        return 'Environmental Services'
    elif any(x in agency_name for x in ['Economic Development', 'Finance', 'Comptroller']):
        return 'Economic and Finance'
    elif any(x in agency_name for x in ['Information Technology', 'Technology and Innovation']):
        return 'Technology and Innovation'
    elif any(x in agency_name for x in ['City Council', 'Public Design']):
        return 'City Governance'
    else:
        return 'Other'

df['Agency Category'] = df['Agency Name'].apply(categorize_agency_reduced)

agency_counts = df['Agency Category'].value_counts()
print(agency_counts)

unique_donation_types = df['Type of Donation'].unique()
print(unique_donation_types)

def categorize_donation_type(donation_type):
    if isinstance(donation_type, str):
        donation_type = donation_type.lower()
        if any(x in donation_type for x in ['money', 'stock', 'funding', 'monetary', 'gift card']):
            return 'Monetary Donation'
        elif any(x in donation_type for x in ['in-kind', 'in kind', 'service']):
            return 'In-Kind Donation'
        elif any(x in donation_type for x in ['glove', 'mask', 'gown', 'ppe', 'face shield', 'coverall', 'hand sanitizer', 'disinfectant', 'thermometer', 'ventilator', 'scrub']):
            return 'Personal Protective Equipment (PPE)'
        else:
            return 'Other'
    else:
        return 'Other'  # Categorize non-string types as 'Other'

df['Donation Type'] = df['Type of Donation'].apply(categorize_donation_type)

donation_category_counts = df['Donation Type'].value_counts()
print(donation_category_counts)

df = df.drop('Type of Donation', axis=1)

df.head()

def clean_donation_value(value):
  if isinstance(value, str):
    value = value.replace('$', '').replace(',', '')
    if value.endswith('-'):
      value = value[:-1]
  try:
    return float(value)
  except:
    return 0.0

df['Value of Donation'] = df['Value of Donation'].apply(clean_donation_value)

df.info()

#Checking for any missing values
print(df.isnull().sum())

df.dropna(subset=['Donation Type'], inplace=True)
df.dropna(subset=['Value of Donation'], inplace=True)
df = df[df['Name of Donor (Individual or Firm)'] != 'No donations to report']

df.info()

"""# **1.1 Data Simulation**"""

df.info()

# Create a dictionary to map donor names to unique IDs
unique_donor_ids = {donor: f'DONOR_{i+1}' for i, donor in enumerate(df['Name of Donor (Individual or Firm)'].unique())}

# Map the donor names in the DataFrame to their unique IDs
df['Unique Identifier'] = df['Name of Donor (Individual or Firm)'].map(unique_donor_ids)

# prompt: find the total unique count of Name of Donor (Individual or Firm) and Unique Identifier column

total_unique_donor_id_count = df[['Unique Identifier']].nunique().sum()
print("Total unique count:", total_unique_donor_id_count)

# prompt: find the total unique count of Name of Donor (Individual or Firm) and Unique Identifier column

total_unique_donor_name_count = df[['Name of Donor (Individual or Firm)']].nunique().sum()
print("Total unique Donor Name count:", total_unique_donor_name_count)

# Find duplicate entries in the Unique Identifier column
duplicates = df['Unique Identifier'].duplicated()

# Count the number of duplicates
num_duplicates = duplicates.sum()

print(f"Number of duplicate Unique Identifiers: {num_duplicates}")

# Find duplicate entries in the Unique Identifier column
duplicates1 = df['Name of Donor (Individual or Firm)'].duplicated()

# Count the number of duplicates
num_duplicates1 = duplicates1.sum()

print(f"Number of duplicate Name of Donor (Individual or Firm): {num_duplicates}")

df.info()

import numpy as np
import pandas as pd

# Step 1: Create a dictionary mapping each unique identifier to a gender
unique_identifiers = df['Unique Identifier'].unique()
# Generate random gender assignments based on the 60% female, 40% male distribution
gender_choices = np.random.choice(['Female', 'Male'], size=len(unique_identifiers), p=[0.6, 0.4])
identifier_to_gender = dict(zip(unique_identifiers, gender_choices))

# Step 2: Map the gender back to the DataFrame
df['Gender'] = df['Unique Identifier'].map(identifier_to_gender)

# Verify the distribution
print(df['Gender'].value_counts(normalize=True))

import numpy as np

# Define the number of unique identifiers
num_identifiers = len(df['Unique Identifier'].unique())

# Generate random ages
# 85% between 18 and 45
ages_85_percent = np.random.randint(18, 46, size=int(num_identifiers * 0.85))
# 15% between 46 and 65
ages_15_percent = np.random.randint(46, 66, size=int(num_identifiers * 0.15))

# Combine the two age distributions
ages = np.concatenate([ages_85_percent, ages_15_percent])
# Shuffle the combined array
np.random.shuffle(ages)

# Create a mapping from unique identifiers to ages
unique_identifiers = df['Unique Identifier'].unique()
identifier_to_age = dict(zip(unique_identifiers, ages))

# Assign Age to the DataFrame
df['Age'] = df['Unique Identifier'].map(identifier_to_age)

# Generate and assign Education Level
education_distribution = np.random.choice(
    ['College', 'High School or Less', 'Masters Level', 'Doctorate'],
    size=len(unique_identifiers),
    p=[0.5, 0.2, 0.2, 0.1]
)
identifier_to_education = dict(zip(unique_identifiers, education_distribution))
df['Education Level'] = df['Unique Identifier'].map(identifier_to_education)

# Generate and assign Employment Status
employment_distribution = np.random.choice(
    ['Full-Time', 'Part-Time', 'Unemployed'],
    size=len(unique_identifiers),
    p=[0.85, 0.1, 0.05]
)
identifier_to_employment = dict(zip(unique_identifiers, employment_distribution))
df['Employment Status'] = df['Unique Identifier'].map(identifier_to_employment)

# Generate and assign Internet Access
internet_distribution = np.random.choice(
    ['Yes', 'No'],
    size=len(unique_identifiers),
    p=[0.85, 0.15]
)
identifier_to_internet = dict(zip(unique_identifiers, internet_distribution))
df['Internet Access'] = df['Unique Identifier'].map(identifier_to_internet)

# Verify the distributions
print(df['Age'].describe())
print(df['Education Level'].value_counts(normalize=True))
print(df['Employment Status'].value_counts(normalize=True))
print(df['Internet Access'].value_counts(normalize=True))

import pandas as pd
import numpy as np

# Create unique identifiers from the 'Unique Identifier' column
unique_identifiers = df['Unique Identifier'].unique()

# Marital Status
marital_status_distribution = np.random.choice(
    ['Single', 'Married', 'Divorced', 'Widowed'],
    size=len(unique_identifiers),
    p=[0.3, 0.6, 0.07, 0.03]
)
identifier_to_marital_status = dict(zip(unique_identifiers, marital_status_distribution))

# Ethnicity
ethnicity_distribution = np.random.choice(
    ['White', 'Hispanic or Latino', 'Black or African American', 'Asian', 'Other'],
    size=len(unique_identifiers),
    p=[0.6, 0.18, 0.12, 0.06, 0.04]
)
identifier_to_ethnicity = dict(zip(unique_identifiers, ethnicity_distribution))

# Location
location_distribution = np.random.choice(
    ['Urban', 'Suburban', 'Rural'],
    size=len(unique_identifiers),
    p=[0.7, 0.2, 0.1]
)
identifier_to_location = dict(zip(unique_identifiers, location_distribution))

# Assign these values to the DataFrame
df['Marital Status'] = df['Unique Identifier'].map(identifier_to_marital_status)
df['Ethnicity'] = df['Unique Identifier'].map(identifier_to_ethnicity)
df['Location'] = df['Unique Identifier'].map(identifier_to_location)

# Verify the distributions
print(df['Marital Status'].value_counts(normalize=True))
print(df['Ethnicity'].value_counts(normalize=True))
print(df['Location'].value_counts(normalize=True))

total_unique_age_count = df[['Age']].nunique().sum()
print("Total unique age count:", total_unique_age_count)

df.info()

from google.colab import files

# Convert the DataFrame to a CSV file
df.to_csv('donations_data.csv', encoding='utf-8', index=False)

# Download the CSV file
files.download('donations_data.csv')

"""# **1.2 EDA**"""

df.head()

df.info()

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

# Group the data by year and agency category and sum the donation values
df_grouped = df.groupby(['Year', 'Agency Category'])['Value of Donation'].sum().unstack()

# Increase figure size to accommodate the legend
fig, ax = plt.subplots(figsize=(8, 6))

# Plot the stacked bar chart
df_grouped.plot(kind='bar', stacked=True, ax=ax)

# Set labels and title
plt.xlabel('Year')
plt.ylabel('Total Donation Value')
plt.title('Total Donation Value by Agency Category Over Time')

# Format the y-axis to avoid scientific notation
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))  # Format y-axis to display full notation

# Adjust the legend size and location
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')

# Adjust the layout to make room for the legend without shrinking the chart
plt.tight_layout()

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.ticker as ticker

# Summarize data by categorical fields
gender_summary = df.groupby('Gender')['Value of Donation'].sum().reset_index()
age_summary = df.groupby('Age')['Value of Donation'].sum().reset_index()
education_summary = df.groupby('Education Level')['Value of Donation'].sum().reset_index()
employment_summary = df.groupby('Employment Status')['Value of Donation'].sum().reset_index()
internet_summary = df.groupby('Internet Access')['Value of Donation'].sum().reset_index()
marital_status_summary = df.groupby('Marital Status')['Value of Donation'].sum().reset_index()
ethnicity_summary = df.groupby('Ethnicity')['Value of Donation'].sum().reset_index()
location_summary = df.groupby('Location')['Value of Donation'].sum().reset_index()

# Visualize total donation value by Gender
plt.figure(figsize=(5, 3))
sns.barplot(x='Gender', y='Value of Donation', data=gender_summary, color='gray')
plt.title('Total Donation Value by Gender')
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))  # Format y-axis to display full notation
plt.show()

# Visualize total donation value by Age
plt.figure(figsize=(15, 3))
sns.barplot(x='Age', y='Value of Donation', data=age_summary, color='gray')
plt.title('Total Donation Value by Age')
plt.xticks(rotation=45)  # Rotate x-axis labels
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))  # Format y-axis to display full notation
plt.show()

# Visualize total donation value by Education Level
plt.figure(figsize=(5, 3))
sns.barplot(x='Education Level', y='Value of Donation', data=education_summary, color='gray')
plt.title('Total Donation Value by Education Level')
plt.xticks(rotation=45)  # Rotate labels if necessary
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))  # Format y-axis to display full notation
plt.show()

# Visualize total donation value by Employment Status
plt.figure(figsize=(5, 3))
sns.barplot(x='Employment Status', y='Value of Donation', data=employment_summary, color='gray')
plt.title('Total Donation Value by Employment Status')
plt.xticks(rotation=45)  # Rotate labels if necessary
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))  # Format y-axis to display full notation
plt.show()

# Visualize total donation value by Internet Access
plt.figure(figsize=(5, 3))
sns.barplot(x='Internet Access', y='Value of Donation', data=internet_summary, color='gray')
plt.title('Total Donation Value by Internet Access')
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))  # Format y-axis to display full notation
plt.show()

# Visualize total donation value by Marital Status
plt.figure(figsize=(5, 3))
sns.barplot(x='Marital Status', y='Value of Donation', data=marital_status_summary, color='gray')
plt.title('Total Donation Value by Marital Status')
plt.xticks(rotation=45)  # Rotate labels if necessary
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))  # Format y-axis to display full notation
plt.show()

# Visualize total donation value by Ethnicity
plt.figure(figsize=(5, 3))
sns.barplot(x='Ethnicity', y='Value of Donation', data=ethnicity_summary, color='gray')
plt.title('Total Donation Value by Ethnicity')
plt.xticks(rotation=90)  # Rotate labels if necessary
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))  # Format y-axis to display full notation
plt.show()

# Visualize total donation value by Location
plt.figure(figsize=(5, 3))
sns.barplot(x='Location', y='Value of Donation', data=location_summary, color='gray')
plt.title('Total Donation Value by Location')
plt.xticks(rotation=90)  # Rotate labels if necessary
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))  # Format y-axis to display full notation
plt.show()

#Example: Analyzing the average spent amount by Agency Category over the years
avg_donation_by_agency_year=df.groupby(['Year', 'Agency Category'])['Value of Donation'].mean().reset_index()
print(avg_donation_by_agency_year)

#Example: Heatmap of Value of Donation by year and Agency Category
pivot_table=df.pivot_table(values='Value of Donation', index='Agency Category', columns='Year', aggfunc='sum')
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, annot=True, fmt='.2f', cmap='Blues')
plt.title('Heatmap of Average Donation by Year and Agency Category')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.ticker as ticker

# Create a box plot of 'Value of Donation'
plt.figure(figsize=(8, 6))
sns.boxplot(y='Value of Donation', data=df)
plt.title('Box Plot of Donation Value')
plt.ylabel('Donation Value')

# Format y-axis to display full notation with a dollar sign
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))

plt.show()

import pandas as pd

# Calculate Q1 (25th percentile) and Q3 (75th percentile) of 'Value of Donation'
Q1 = df['Value of Donation'].quantile(0.25)
Q3 = df['Value of Donation'].quantile(0.75)
IQR = Q3 - Q1

# Define outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Find outliers
outliers = df[(df['Value of Donation'] < lower_bound) | (df['Value of Donation'] > upper_bound)]

# Display the outliers
print(f'Number of outliers: {len(outliers)}')
print(outliers.head())

import pandas as pd

# Calculate Q1 (25th percentile) and Q3 (75th percentile) of 'Value of Donation'
Q1 = df['Value of Donation'].quantile(0.25)
Q3 = df['Value of Donation'].quantile(0.75)
IQR = Q3 - Q1

# Define boundaries for extreme outliers (using 3 times the IQR)
lower_bound_extreme = Q1 - 3 * IQR
upper_bound_extreme = Q3 + 3 * IQR

# Find extreme outliers
extreme_outliers = df[(df['Value of Donation'] < lower_bound_extreme) | (df['Value of Donation'] > upper_bound_extreme)]

# Display the extreme outliers
print(f'Number of extreme outliers: {len(extreme_outliers)}')
print(extreme_outliers.head())

import pandas as pd
import numpy as np

# Calculate Q1 (25th percentile) and Q3 (75th percentile) of 'Value of Donation'
Q1 = df['Value of Donation'].quantile(0.25)
Q3 = df['Value of Donation'].quantile(0.75)
IQR = Q3 - Q1

# Define the boundaries for Winsorizing
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Apply Winsorizing: cap the values at the lower and upper bounds
df['Value of Donation Winsorized'] = np.where(
    df['Value of Donation'] < lower_bound,
    lower_bound,
    np.where(df['Value of Donation'] > upper_bound,
             upper_bound,
             df['Value of Donation'])
)

# Verify changes
print(f'Number of entries: {len(df)}')
print(f'Number of unique values in Winsorized column: {df["Value of Donation Winsorized"].nunique()}')
print(df[['Value of Donation', 'Value of Donation Winsorized']].describe())

# Optional: Display outliers after Winsorizing
winsorized_outliers = df[(df['Value of Donation Winsorized'] < lower_bound) | (df['Value of Donation Winsorized'] > upper_bound)]
print(f'Number of Winsorized outliers: {len(winsorized_outliers)}')
print(winsorized_outliers.head())

# prompt: find the range of values in Value of Donation Winsorized

min_value = df['Value of Donation'].min()
max_value = df['Value of Donation'].max()

print(f"Range of Donation Values: {min_value} to {max_value}")

# prompt: find the range of values in Value of Donation Winsorized

min_value = df['Value of Donation Winsorized'].min()
max_value = df['Value of Donation Winsorized'].max()

print(f"Range of Winsorized Donation Values: {min_value} to {max_value}")

# Set up the matplotlib figure
plt.figure(figsize=(14, 6))

# Plot density plot of original data
plt.subplot(1, 2, 1)
sns.kdeplot(df['Value of Donation'], fill=True, color='gray')
plt.title('Density Plot of Value of Donation (Original)')
plt.xlabel('Value of Donation')
plt.ylabel('Density')

# Plot density plot of Winsorized data
plt.subplot(1, 2, 2)
sns.kdeplot(df['Value of Donation Winsorized'], fill=True, color='blue')
plt.title('Density Plot of Value of Donation (Winsorized)')
plt.xlabel('Value of Donation')
plt.ylabel('Density')

plt.tight_layout()
plt.show()

import matplotlib.ticker as ticker

# Set up the matplotlib figure
plt.figure(figsize=(10, 6))

# Plot box plot of original data
plt.subplot(1, 2, 1)
sns.boxplot(y=df['Value of Donation'], color='blue')
plt.title('Box Plot of Value of Donation (Original)')
plt.ylabel('Value of Donation')
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))

# Plot box plot of Winsorized data
plt.subplot(1, 2, 2)
sns.boxplot(y=df['Value of Donation Winsorized'], color='blue')
plt.title('Box Plot of Value of Donation (Winsorized)')
plt.ylabel('Value of Donation')
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))

plt.tight_layout()
plt.show()

"""# **1.3 Feature Engineering**"""

df.info()

# prompt: find the range of values in Value of Donation Winsorized

min_value = df['Value of Donation Winsorized'].min()
max_value = df['Value of Donation Winsorized'].max()

print(f"Range of Winsorized Donation Values: {min_value} to {max_value}")

dffe = df.copy()

import pandas as pd

# Adjust the bin edges to ensure all values are included
bins = [0, 5000, 50000, 380350.01]
labels = ['Low', 'Medium', 'High']

# Create the segmentation
dffe['Donation Segment'] = pd.cut(dffe['Value of Donation Winsorized'], bins=bins, labels=labels, right=False)

# Display the first few rows to verify the segmentation
print(dffe[['Value of Donation Winsorized', 'Donation Segment']].head())

dffe

dffe.info()

dffe['Donation Frequency'] = dffe.groupby('Name of Donor (Individual or Firm)')['Value of Donation'].transform('count')
high_value_threshold = 50000
dffe['High Value Donor'] = dffe['Value of Donation Winsorized'].apply(lambda x: 1 if x >= high_value_threshold else 0)
current_year = dffe['Year'].max()
dffe['Donation Recency'] = current_year - dffe['Year']
dffe['Engagement Score'] = (dffe['Donation Frequency'] + (1 / (1 + dffe['Donation Recency'])) + dffe['Value of Donation Winsorized']).rank(ascending=False)

dffe.head()

# prompt: find the range of Engagement Score

min_value = dffe['Engagement Score'].min()
max_value = dffe['Engagement Score'].max()

print(f"Range of Engagement Score: {min_value} to {max_value}")

dffe['Scaled Engagement Score'] = 100 * (dffe['Engagement Score'] - dffe['Engagement Score'].min()) / (dffe['Engagement Score'].max() - dffe['Engagement Score'].min())

dffe

# prompt: find the range of Scaled Engagement Score

min_value = dffe['Scaled Engagement Score'].min()
max_value = dffe['Scaled Engagement Score'].max()

print(f"Range of Scaled Engagement Score: {min_value} to {max_value}")

dffe['Time Since Last Donation'] = current_year - dffe['Year']

current_year = dffe['Year'].max()
churn_threshold = 2  # Define churn threshold (e.g., if no donation in last 2 years)
dffe['Churned'] = (current_year - dffe['Year'] > churn_threshold).astype(int)

dffe.info()

import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta


# Function to generate a random date within a given year
def random_date_in_year(year):
    start_date = datetime(year, 1, 1)
    end_date = datetime(year, 12, 31)
    return start_date + timedelta(days=random.randint(0, (end_date - start_date).days))

# Apply the random date generation to each row and create a new column 'Date'
dffe['Date'] = dffe['Year'].apply(random_date_in_year)

dffe.info()

"""# **2 Regression Analysis For Donation Value**"""

dfRe = dffe.copy()

# Drop the redundant columns
dfRe.drop(columns=['Value of Donation', 'Year'], inplace=True)

# Rename the 'Value of Donation Winsorized' column to 'Donation Value'
dfRe.rename(columns={'Value of Donation Winsorized': 'Donation Value'}, inplace=True)

dfRe.info()

# Isolate numerical columns (float64 and int64)
numerical_and_datetime_columns = dfRe.select_dtypes(include=['float64', 'int64', 'datetime64[ns]'])

# Display the first few rows of these numerical columns
print(numerical_and_datetime_columns.head())

# Optionally, check the column names
print(numerical_and_datetime_columns.columns)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Create a copy of the DataFrame and label it as dfprocessed1
dfprocessed1 = dfRe.copy()

# Drop the 'Unique Identifier', 'Name of Donor (Individual or Firm)', and 'Agency Name' columns
dfprocessed1.drop(columns=['Unique Identifier', 'Name of Donor (Individual or Firm)', 'Agency Name'], inplace=True)

# Apply One-Hot Encoding to nominal categorical columns
dfprocessed1 = pd.get_dummies(dfprocessed1, columns=[
    'Agency Category', 'Donation Type',
    'Employment Status', 'Internet Access', 'Marital Status',
    'Ethnicity', 'Location', 'Donation Segment'
], drop_first=False)

# Label Encoding for 'Education Level' as it is ordinal
label_encoder = LabelEncoder()
dfprocessed1['Education Level'] = label_encoder.fit_transform(dfprocessed1['Education Level'])

# One-Hot Encoding for 'Gender' since it might be binary
dfprocessed1 = pd.get_dummies(dfprocessed1, columns=['Gender'], drop_first=False)

# Identify Boolean columns
boolean_columns = dfprocessed1.select_dtypes(include=['bool']).columns

# Convert Boolean columns to integers
dfprocessed1[boolean_columns] = dfprocessed1[boolean_columns].astype(int)

# Ensure no categorical columns remain that should be converted
dfprocessed1 = dfprocessed1.convert_dtypes()

dfprocessed1.info()

# Remove unwanted columns
dfprocessed1.drop(columns=[
    'Donation Type_Other',
    'Gender_Male',
    'Internet Access_No',
    'Donation Segment_Medium',
    'Location_Suburban',
    'Ethnicity_Other',
    'Marital Status_Divorced',
    'Employment Status_Part-Time',
    'Agency Category_Other'
], inplace=True)

dfprocessed1.info()

# Set display options to show all columns
pd.set_option('display.max_columns', None)
dfprocessed1.head(5)

dfprocessed2 = dfprocessed1.copy()

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Initialize StandardScaler
scaler = StandardScaler()

# List of numerical columns to scale
numerical_columns = [
    'Age', 'Donation Value', 'Donation Frequency',
    'High Value Donor', 'Donation Recency', 'Engagement Score',
    'Scaled Engagement Score', 'Time Since Last Donation'
]

# Apply scaling to numerical columns
dfprocessed2[numerical_columns] = scaler.fit_transform(dfprocessed2[numerical_columns])

# Extract date features from 'Date' column
dfprocessed2['Year'] = dfprocessed2['Date'].dt.year
dfprocessed2['Month'] = dfprocessed2['Date'].dt.month
dfprocessed2['Day'] = dfprocessed2['Date'].dt.day
dfprocessed2['Day of Week'] = dfprocessed2['Date'].dt.dayofweek
dfprocessed2['Quarter'] = dfprocessed2['Date'].dt.quarter

# Drop the original 'Date' column if no longer needed
dfprocessed2.drop(columns=['Date'], inplace=True)

dfprocessed2.info()

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Compute the correlation matrix
correlation_matrix = dfprocessed2.corr()

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(35, 20))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu', fmt='.2f', vmin=-1, vmax=1, square=True, linewidths=0.5, annot_kws={"size": 10})

# Add title
plt.title('Correlation Matrix (Lower Triangle)')

# Show the plot
plt.show()

import pandas as pd
import numpy as np

# Compute the correlation matrix
correlation_matrix = dfprocessed2.corr()

# Create a mask for the upper triangle to avoid duplicate pairs
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Get the correlation matrix without the upper triangle
corr_long = correlation_matrix.where(~mask).stack().reset_index()
corr_long.columns = ['Feature1', 'Feature2', 'Correlation']

# Remove self-correlations (where Feature1 == Feature2)
corr_long = corr_long[corr_long['Feature1'] != corr_long['Feature2']]

# Sort the correlations by absolute value in descending order
corr_long['AbsCorrelation'] = corr_long['Correlation'].abs()
top_corr = corr_long.sort_values(by='AbsCorrelation', ascending=False).head(10)

# Display the top correlated feature pairs
print(top_corr)

dfprocessed2.info()

# Impute missing values with the mean
dfprocessed2['Age'] = dfprocessed2['Age'].fillna(dfprocessed2['Age'].mean())

"""# **2.1 Simple Regression Model**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Define target variable and features
X = dfprocessed2.drop(columns=['Donation Value'])
y = dfprocessed2['Donation Value']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.linear_model import LinearRegression

# Initialize the Linear Regression model
model = LinearRegression()

# Train the model
model.fit(X_train_scaled, y_train)
# Make predictions
y_pred = model.predict(X_test_scaled)

from sklearn.metrics import mean_squared_error, r2_score

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R^2 Score: {r2:.2f}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Create a copy of the dataframe
dfprocessed2_copy = dfprocessed2.copy()

# Drop specific columns and columns starting with 'Agency'
columns_to_drop = ['Year', 'Month', 'Day', 'Day of Week', 'Quarter', 'Churned',
                    'Agency Category_Borough Leadership', 'Agency Category_City Governance',
                    'Agency Category_City Leadership']

# Add columns starting with 'Agency' to the list of columns to drop
columns_to_drop += [col for col in dfprocessed2_copy.columns if col.startswith('Agency')]

# Drop the specified columns
dfprocessed2_copy = dfprocessed2_copy.drop(columns=columns_to_drop)


# Define target variable and features
X = dfprocessed2_copy.drop(columns=['Donation Value'])
y = dfprocessed2_copy['Donation Value']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print results
print(f"Mean Squared Error: {mse:.2f}")
print(f"R^2 Score: {r2:.2f}")

# Get feature names
feature_names = X.columns

# Get model coefficients
coefficients = model.coef_

# Create a DataFrame for feature importance
feature_importance = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': coefficients
}).sort_values(by='Coefficient', ascending=False)

print(feature_importance)

import matplotlib.pyplot as plt

# Calculate residuals
residuals = y_test - y_pred

# Plot residuals
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs Predicted Values')
plt.show()

from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')

# Print cross-validation results
print(f"Cross-Validation MSE Scores: {-cv_scores}")
print(f"Mean Cross-Validation MSE: {-cv_scores.mean()}")

from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import GridSearchCV

# Initialize Ridge and Lasso models
ridge = Ridge()
lasso = Lasso()

# Set up hyperparameter grids
param_grid = {
    'alpha': [0.01, 0.1, 1, 10, 100]
}

# Grid search for Ridge
grid_search_ridge = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search_ridge.fit(X_train_scaled, y_train)

# Grid search for Lasso
grid_search_lasso = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search_lasso.fit(X_train_scaled, y_train)

# Print best parameters and scores
print(f"Best Ridge Parameters: {grid_search_ridge.best_params_}")
print(f"Best Ridge Score: {-grid_search_ridge.best_score_}")

print(f"Best Lasso Parameters: {grid_search_lasso.best_params_}")
print(f"Best Lasso Score: {-grid_search_lasso.best_score_}")

"""# **2.2 Polynomial Regression Model**"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression

# Define polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)

# Create a pipeline with polynomial features and linear regression
model_poly = make_pipeline(poly, LinearRegression())

# Fit the model
model_poly.fit(X_train_scaled, y_train)

# Make predictions
y_pred_poly = model_poly.predict(X_test_scaled)

# Evaluate the model
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)

print(f"Polynomial Regression MSE: {mse_poly}")
print(f"Polynomial Regression R^2 Score: {r2_poly}")

import matplotlib.pyplot as plt

# Fit the polynomial model
model_poly.fit(X_train_scaled, y_train)

# Make predictions
y_pred_poly = model_poly.predict(X_test_scaled)

# Calculate residuals
residuals_poly = y_test - y_pred_poly

# Plot residuals
plt.scatter(y_pred_poly, residuals_poly, color='blue', edgecolor='k', alpha=0.7)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot for Polynomial Regression')
plt.show()

from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures

# Define polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
model_poly = make_pipeline(poly, LinearRegression())

# Perform cross-validation
cv_scores = cross_val_score(model_poly, X, y, cv=5, scoring='neg_mean_squared_error')

# Print cross-validation results
print(f"Cross-Validation MSE Scores: {-cv_scores}")
print(f"Mean Cross-Validation MSE: {-cv_scores.mean()}")

"""# **2.3 Ridge Regression Model**"""

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler

# Define target variable and features
X = dfprocessed2.drop(columns=['Donation Value'])
y = dfprocessed2['Donation Value']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train Ridge Regression model
ridge_model = Ridge(alpha=10)  # You can tune alpha based on previous results
ridge_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_ridge = ridge_model.predict(X_test_scaled)

# Evaluate Ridge Regression
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

# Perform cross-validation
cv_scores_ridge = cross_val_score(ridge_model, X, y, cv=5, scoring='neg_mean_squared_error')

print(f"Ridge Regression MSE: {mse_ridge}")
print(f"Ridge Regression R^2 Score: {r2_ridge}")
print(f"Cross-Validation MSE Scores: {-cv_scores_ridge}")
print(f"Mean Cross-Validation MSE: {-cv_scores_ridge.mean()}")

"""# **2.4 Random Forest Model**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
# Define target variable and features
X = dfprocessed2.drop(columns=['Donation Value'])
y = dfprocessed2['Donation Value']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

# Calculate Mean Squared Error and R^2 Score
mse_rf = mean_squared_error(y_test, y_pred)
r2_rf = r2_score(y_test, y_pred)

print(f"Random Forest MSE: {mse_rf}")
print(f"Random Forest R^2 Score: {r2_rf}")

# Perform cross-validation
cv_scores_rf = cross_val_score(rf_model, X, y, cv=5, scoring='neg_mean_squared_error')

print(f"Random Forest Cross-Validation MSE Scores: {-cv_scores_rf}")
print(f"Mean Random Forest Cross-Validation MSE: {-cv_scores_rf.mean()}")

"""# **3 Clustering Segments**

# **3.1 Donation Behavior Segmentation**
"""

dfRe.info()

dfCl = dfRe.copy()

dfCl.info()

dfCl.head()

# prompt: drop the null row in the missing value of age in df dfcl

dfCl.dropna(subset=['Age'], inplace=True)

dfCl = dfCl.drop(columns=['Date'])

# Check for missing values in each column
missing_values = dfCl.isnull().sum()

# Display columns with missing values
missing_columns = missing_values[missing_values > 0]
print(missing_columns)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Assuming dfCl is your DataFrame
features = ['Age', 'Donation Value', 'Donation Frequency', 'Scaled Engagement Score', 'Time Since Last Donation']

# Scaling the features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(dfCl[features])

# Reduce to 2 dimensions with PCA for visualization
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_scaled)

# Optionally, visualize the data after PCA
plt.figure(figsize=(8, 6))
plt.scatter(df_pca[:, 0], df_pca[:, 1], alpha=0.5)
plt.title('PCA - 2D Projection')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

# Calculate WCSS for different values of K
wcss = []
for k in range(1, 11):  # Adjust range as needed
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_scaled)  # Use scaled data for KMeans
    wcss.append(kmeans.inertia_)

# Plot the results
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.title('Elbow Method for Optimal K')
plt.show()

# Choose the optimal number of clusters from the Elbow Method
optimal_k = 4

# Apply K-means clustering
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(df_scaled)  # Using scaled data for clustering

# Add the cluster labels to your original DataFrame
dfCl['Cluster'] = clusters

# Dimensionality Reduction for Visualization
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_scaled)
dfCl['PCA1'] = df_pca[:, 0]
dfCl['PCA2'] = df_pca[:, 1]

# Visualize the clusters
plt.figure(figsize=(10, 6))
plt.scatter(dfCl['PCA1'], dfCl['PCA2'], c=dfCl['Cluster'], cmap='viridis', s=50)
plt.colorbar(label='Cluster')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('K-Means Clustering with PCA')
plt.show()

# Calculate the silhouette score
score = silhouette_score(df_scaled, clusters)
print(f'Silhouette Score: {score}')

# Select only numeric columns for clustering summary
numeric_columns = dfCl.select_dtypes(include=[np.number])

# Add the 'Cluster' column back to the numeric data
numeric_columns['Cluster'] = dfCl['Cluster']

# Group by the 'Cluster' column and calculate the mean
cluster_summary = numeric_columns.groupby('Cluster').mean()

# Display the summary
print("Numeric Columns Summary:")
print(cluster_summary)

"""# **3.2 Donor Segmentation**"""

dfClDonor = dfRe.copy()

dfClDonor.info()

# prompt: for the datafram dfClDonor drop Agency Name, Name of Donor (Individual or Firm), Unique Identifier, Date

dfClDonor = dfClDonor.drop(columns=['Agency Name', 'Name of Donor (Individual or Firm)', 'Unique Identifier', 'Date'])

dfClDonor.info()

# prompt: drop the null row in the missing value of age in df dfcl

dfClDonor.dropna(subset=['Age'], inplace=True)

import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Define the demographic columns
demographic_features = [
    'Gender', 'Age', 'Education Level', 'Employment Status',
    'Internet Access', 'Marital Status', 'Ethnicity', 'Location'
]

# Separate features
df_demo = dfClDonor[demographic_features]

# Define categorical features
categorical_features = ['Gender', 'Education Level', 'Employment Status',
                         'Internet Access', 'Marital Status', 'Ethnicity', 'Location']

# Fill missing values in categorical columns
df_demo[categorical_features] = df_demo[categorical_features].fillna(df_demo[categorical_features].mode().iloc[0])

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['Age']),
        ('cat', OneHotEncoder(), categorical_features)
    ])

# Apply preprocessing
X_preprocessed = preprocessor.fit_transform(df_demo)

# Calculate WCSS for different values of K
wcss = []
for k in range(1, 11):  # Adjust the range as needed
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_preprocessed)
    wcss.append(kmeans.inertia_)

# Plot the results
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.title('Elbow Method for Optimal K')
plt.show()

# Assuming you determined k=3 from the Elbow Method
optimal_k = 3

# Apply K-Means clustering
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
dfClDonor['Demographic Cluster'] = kmeans.fit_predict(X_preprocessed)

# Dimensionality Reduction for Visualization
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_preprocessed)
dfClDonor['PCA1'] = X_pca[:, 0]
dfClDonor['PCA2'] = X_pca[:, 1]

# Visualize clusters
plt.figure(figsize=(10, 6))
plt.scatter(dfClDonor['PCA1'], dfClDonor['PCA2'], c=dfClDonor['Demographic Cluster'], cmap='viridis', s=50)
plt.colorbar(label='Cluster')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('Donor Demographic Segmentation Clusters')
plt.show()

score = silhouette_score(X_preprocessed, dfClDonor['Demographic Cluster'])
print(f'Silhouette Score: {score}')

# Separate numeric and non-numeric columns
numeric_columns = dfClDonor.select_dtypes(include=['float64', 'int64']).columns
non_numeric_columns = dfClDonor.select_dtypes(include=['object', 'category']).columns

# Calculate mean for numeric columns
numeric_summary = dfClDonor.groupby('Demographic Cluster')[numeric_columns].mean()

# For non-numeric columns, summarize by count or mode
non_numeric_summary = dfClDonor.groupby('Demographic Cluster')[non_numeric_columns].agg(lambda x: x.mode()[0] if not x.mode().empty else 'No Mode')

print("Numeric Columns Summary:")
print(numeric_summary)

print("\nNon-Numeric Columns Summary:")
print(non_numeric_summary)

"""# **4 Time Series Analysis**

# **4.1 Simple Linear Regression**
"""

dfRe.info()

dfSRTS = dfRe.copy()

dfSRTS.info()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Sample DataFrame dfSRTS (assuming it's already loaded)
# Convert 'Date' to datetime format
dfSRTS['Date'] = pd.to_datetime(dfSRTS['Date'], format='%d/%m/%Y')

# Sort the DataFrame by date
dfSRTS = dfSRTS.sort_values(by='Date')

# Extract the target variable and feature
dfSRTS['Date_ordinal'] = dfSRTS['Date'].map(pd.Timestamp.toordinal)

# Define X and y
X = dfSRTS[['Date_ordinal']]
y = dfSRTS['Donation Value']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train the model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predict on the test set
y_pred = linear_model.predict(X_test)

# Model Summary
print("Model Coefficients:", linear_model.coef_)
print("Model Intercept:", linear_model.intercept_)
print("R-squared Score:", r2_score(y_test, y_pred))
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))

# Plot the results
plt.figure(figsize=(12, 8))

# Plot actual values
plt.scatter(dfSRTS['Date'], dfSRTS['Donation Value'], color='gray', label='Actual')

# Plot predicted values
plt.plot(dfSRTS['Date'], linear_model.predict(dfSRTS[['Date_ordinal']]), color='red', label='Predicted')

plt.xlabel('Date')
plt.ylabel('Donation Value')
plt.title('Linear Regression on Time Series Data')
plt.legend()
plt.show()

dfSRTSEnhanced = dfRe.copy()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures

# Assuming dfSRTSEnhanced is already loaded

# Convert 'Date' to datetime format
dfSRTSEnhanced['Date'] = pd.to_datetime(dfSRTSEnhanced['Date'], format='%d/%m/%Y')

# Extract date components
dfSRTSEnhanced['Year'] = dfSRTSEnhanced['Date'].dt.year
dfSRTSEnhanced['Month'] = dfSRTSEnhanced['Date'].dt.month
dfSRTSEnhanced['Day'] = dfSRTSEnhanced['Date'].dt.day
dfSRTSEnhanced['DayOfWeek'] = dfSRTSEnhanced['Date'].dt.dayofweek

# # Create rolling averages (e.g., 7-day rolling average)
# dfSRTSEnhanced['Rolling_Avg'] = dfSRTSEnhanced['Donation Value'].rolling(window=7).mean().fillna(method='bfill')

# Define features
features = ['Year', 'Month', 'Day', 'DayOfWeek']
X = dfSRTSEnhanced[features]
y = dfSRTSEnhanced['Donation Value']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and fit a Linear Regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predict
y_pred = linear_model.predict(X_test)

# Model Summary
print("Model Coefficients:", linear_model.coef_)
print("Model Intercept:", linear_model.intercept_)
print("R-squared Score:", r2_score(y_test, y_pred))
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))

# Plotting results
plt.figure(figsize=(12, 8))

# Scatter plot of actual values
plt.scatter(dfSRTSEnhanced['Date'], dfSRTSEnhanced['Donation Value'], color='gray', label='Actual')

# Predict over the entire range
plt.plot(dfSRTSEnhanced['Date'], linear_model.predict(dfSRTSEnhanced[features]), color='red', label='Predicted')

plt.xlabel('Date')
plt.ylabel('Donation Value')
plt.title('Linear Regression with Enhanced Features')
plt.legend()
plt.show()

"""# **4.2 ARIMA Time Series**"""

dfATS = dfRe.copy()

dfATS.info()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import adfuller

# Sample DataFrame dfATS (assuming it's already loaded)
# Convert 'Date' to datetime format
dfATS['Date'] = pd.to_datetime(dfATS['Date'], format='%d/%m/%Y')

# Set 'Date' as the index
dfATS.set_index('Date', inplace=True)

# Sort the DataFrame by date
dfATS = dfATS.sort_index()

# Check for stationarity using the Augmented Dickey-Fuller test
result = adfuller(dfATS['Donation Value'].dropna())
print('ADF Statistic:', result[0])
print('p-value:', result[1])

# If the p-value is high (greater than 0.05), the data is not stationary and may need differencing.

# Ensure there are no duplicate dates
dfATS = dfATS[~dfATS.index.duplicated(keep='first')]

# Set frequency for the date index
dfATS = dfATS.asfreq('D')

# Determine p and q using ACF and PACF plots
plt.figure(figsize=(12, 6))
plt.subplot(121)
plot_acf(dfATS['Donation Value'].dropna(), ax=plt.gca())
plt.title('ACF Plot')

plt.subplot(122)
plot_pacf(dfATS['Donation Value'].dropna(), ax=plt.gca())
plt.title('PACF Plot')

plt.show()

# Fit the ARIMA model (order can be adjusted based on ACF and PACF results)
model = ARIMA(dfATS['Donation Value'], order=(1, 0, 1))  # Example order, adjust as needed
model_fit = model.fit()

# Print the summary of the model
print(model_fit.summary())

# Forecasting
forecast_steps = 365  # Number of periods to forecast
forecast = model_fit.forecast(steps=forecast_steps)

# Print the forecasted values
print("Forecasted Values:")
print(forecast)

# Generate a date range for the forecast
forecast_index = pd.date_range(start=dfATS.index[-1], periods=forecast_steps + 1, inclusive='right')

# Plot the historical data and the forecast
plt.figure(figsize=(12, 6))
plt.plot(dfATS.index, dfATS['Donation Value'], label='Historical Data')
plt.plot(forecast_index, forecast, color='red', linestyle='--', label='Forecast')
plt.title('Forecast of Donation Value')
plt.xlabel('Date')
plt.ylabel('Donation Value')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter

# Continue from your existing code

# Resample the historical data to monthly sums
historical_monthly = dfATS['Donation Value'].resample('M').sum()

# Resample the forecasted daily data to monthly sums
forecast_daily = pd.Series(forecast, index=forecast_index)
forecast_monthly = forecast_daily.resample('M').sum()

# Generate the corresponding date range for the monthly forecast data
forecast_monthly_index = pd.date_range(start=forecast_daily.index[0], periods=len(forecast_monthly), freq='M')

# Plot the historical monthly aggregated data and the forecasted monthly data
plt.figure(figsize=(12, 6))
plt.plot(historical_monthly.index, historical_monthly, label='Historical Monthly Data')
plt.plot(forecast_monthly_index, forecast_monthly, color='red', linestyle='--', label='Monthly Aggregated Forecast')

# Formatting the y-axis to display values as dollars
formatter = ScalarFormatter()
formatter.set_scientific(False)
plt.gca().yaxis.set_major_formatter(formatter)

plt.title('Historical and Forecasted Monthly Aggregated Donation Value')
plt.xlabel('Date')
plt.ylabel('Donation Value')
plt.legend()
plt.show()